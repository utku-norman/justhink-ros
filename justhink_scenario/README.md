# JUSThink Scenario

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Overview

This repository contains the [ROS] package to run the pedagogical scenario that contains a human-robot collaborative learning activity for school children, named [JUSThink](https://www.epfl.ch/labs/chili/index-html/research/animatas/justhink/). The scenario aims to improve their computational thinking skills by applying abstract and algorithmic reasoning to solve an unfamiliar problem on networks.

The pedagogical scenario consists of a sequence of activities: cover, introduction, tutorial, five individual activities (pretest), two collaborative activities, five individual activities (posttest), and a goodbye.

* In an individual activity, a human learner is given a network of gold mines with possible positions for railway tracks, where each track if it is built connects one mine to another. The cost of each track is visible. The goal is to collect the gold by connecting the gold mines to each other, while spending as little as possible to build the tracks.
* In a collaborative activity, the human and the robot as (same-status) peers collaboratively construct a solution to this problem by deciding together which tracks to build, and submit it as their solution to the system. They take turns in suggesting to select a specific connection, where the other either agrees or disagrees with this suggestion. A track will be built only if it is suggested by one and accepted by the other.

A human learner participates in the pedagogical scenario through an application ([justhink_scenario]). The robot behaviour is generated by [justhink_agent] and manifested (on e.g. [QTrobot]) by [justhink_robot]. These are [ROS] nodes that communicate via the custom ROS messages and services defined in [justhink_msgs].

**Keywords:** artificial intelligence, human-robot interaction, mutual understanding, collaborative learning, computational thinking

### License

The whole package is under MIT License, see [LICENSE](LICENSE).

This README is based on the project [ros_best_practices](https://github.com/leggedrobotics/ros_best_practices), Copyright 2015-2017, PÃ©ter Fankhauser. It is licensed under the BSD 3-Clause Clear License. See [doc/LICENSE](doc/LICENSE) for additional details.

**Author: Utku Norman<br />
Affiliation: [CHILI Lab, EPFL](https://www.epfl.ch/labs/chili/)<br />
Maintainer: Utku Norman, utku.norman@epfl.ch**

The [justhink_scenario] package has been tested under [ROS] Noetic on Ubuntu 20.04.
This is research code, expect that it changes often and any fitness for a particular purpose is disclaimed.


![](doc/collab_activity.jpg)


### Publications

If you use this work in an academic context, please cite the following publication(s):

* U. Norman, B. Bruno, and P. Dillenbourg, **Mutual Modelling Ability for a Humanoid Robot: How can it improve my learning as we solve a problem together?,** in Robots for Learning Workshop in 16th annual IEEE/ACM Conference on Human-Robot Interaction (HRI 2021). ([PDF](http://infoscience.epfl.ch/record/283614))

        @conference{norman_mutual_2021,
        	author = {Norman, Utku and Bruno, Barbara and Dillenbourg, Pierre},
        	booktitle = {Robots for Learning Workshop in 16th annual {IEEE}/{ACM} Conference on Human-Robot Interaction ({HRI} 2021)},
        	title = {Mutual Modelling Ability for a Humanoid Robot: How can it improve my learning as we solve a problem together?},
        	url = {http://infoscience.epfl.ch/record/283614},
        	year = {2021},
        }


## Installation

### Building from Source

#### Dependencies

* [Robot Operating System (ROS)](http://wiki.ros.org) (middleware for robotics) for the communication between this ([justhink_scenario]) application's node and the robot behaviour node from [justhink_robot]
* [justhink_world](https://github.com/utku-norman/justhink_world) to represent an activity as a world/problem with a state (that depends on [pomdp_py](https://h2r.github.io/pomdp-py/html/), [networkx](https://networkx.org/), [pyglet](https://pyglet.readthedocs.io/en/latest/), [importlib_resources](https://importlib-resources.readthedocs.io/en/latest/), and [pqdict](https://pypi.org/project/pqdict/))
* [justhink_msgs] for the custom ROS headers

#### Building

1) [Install ROS Noetic](http://wiki.ros.org/Installation/).

2) [Create a ROS workspace for catkin](http://wiki.ros.org/catkin/Tutorials/create_a_workspace) (e.g. `~/catkin_ws`):
```
# Load the default workspace.
source /opt/ros/noetic/setup.bash

# Create and overlay your catkin workspace.
mkdir -p ~/catkin_ws/src
cd ~/catkin_ws/
source devel/setup.bash
```

3) Install the [justhink_world] Python package by following its [README](https://github.com/utku-norman/justhink_world/#readme).

4) Activate the [virtual environment](https://docs.python.org/3/tutorial/venv.html) that the JUSThink World is installed in:
```
# JUSTHINK_WORLD_DIR points to the location of the justhink_world source code.
source $JUSTHINK_WORLD_DIR/.venv/bin/activate
```

5) Install Python dependencies for running ROS in the virtual environment:
```
pip install pyyaml rospkg empy
```

6) Install the [justhink_msgs] ROS package that contains the custom headers by following its [README](https://github.com/utku-norman/justhink_msgs/#readme).

7) Clone this [justhink_scenario] ROS package inside the 'src' folder of the catkin workspace:
```
cd ~/catkin_ws/src
git clone https://github.com/utku-norman/justhink_scenario.git
```

8) Build this package with ROS (this also installs the justhink_scenario Python package):
```
cd ~/catkin_ws
catkin build justhink_scenario
source devel/setup.bash
```

If you receive an error `catkin: not found`, make sure you have sourced the ROS environment (i.e. `source ~/catkin_ws/devel/setup.bash` and `source /opt/ros/noetic/setup.bash`), and you have installed the catkin tools by `sudo apt-get install python-catkin-tools`.

9) Check the installation by running the following in a Python interpreter:
```
from justhink_scenario import show_scenario
```

## Usage

1) In a terminal, start the 'roscore':
```
roscore
```

2) In another terminal, run the main node with:
```
# Activate the virtual environment you created for justhink_world.
source $JUSTHINK_WORLD_DIR/.venv/bin/activate

# Set the namespace for the node.
export ROS_NAMESPACE=env

# Launch the app.
rosrun justhink_scenario run_scenario.py
```

* Use `CTRL+LEFT` and `CTRL+RIGHT` keys to navigate to the previous or the next activity, respectively.
* Press `A` while navigating to skip/suppress activity change animation.
* Press `SHIFT` while navigating to skip/suppress publishing activity change (for the robot not to know about it and to navigate quickly to an activity).
* Press `CTRL+SHIFT+P` to publish the current activity for the robot to be aware about it.
* Press `CTRL+P` to toggle pause, and `CTRL+TAB` to toggle the role between the human and the robot.
* Press `CTRL+ESCAPE` to quit.

Note that the collaborative part need the robot node running.


### Running with a touch screen

#### Mapping the touch interface onto the touch screen

Check the name of the touch controller. With iiyama, it is "USBest Technology SiS HID Touch Controller"
```
xinput
```

2) Check the name of the screen, e.g. in mine it is "DP-3"
```
xrandr -q
```

3) Map the touch controller to the screen, e.g., if it is DP-3 from the previous step:
```
xinput map-to-output "USBest Technology SiS HID Touch Controller" DP-3
```

#### Hiding the cursor on touch events

Install the fork of unclutter that hides the cursor for touch only (The default unclutter from apt does not have "-touch".)
```
sudo apt install asciidoc libev-dev libxslt1-dev docbook-xsl xsltproc libxml2-utils    # Prerequisites
git clone https://github.com/nowrep/unclutter-xfixes.git
cd unclutter-xfixes
make
sudo make install
```

5) Run unclutter on a separate terminal. Touch on the screen will not show cursor.
```
unclutter -touch
```

#### Rotating the screen by 180 degrees
To prevent the power button being pressed accidentally (normally bottom right corner, if rotated top left corner)

1) In Display setting of Ubuntu, change Rotation to 180 degrees.

2) Remap the touch upside-down.
```
xinput set-prop "USBest Technology SiS HID Touch Controller" --type=float "Coordinate Transformation Matrix" 0 -1 1 1 0 0 0 0 1
```


## Nodes

### situation

Launches the JUSThink application for the human to interact with the pedagogical scenario.


#### Subscribed Topics

* **`/env/situation/act`** ([[justhink_msgs/Action]](https://github.com/utku-norman/justhink_msgs/blob/main/msg/Action.msg))

	Executes an action in the activity for the robot to take actions like suggesting a connection etc. For example, you can execute an action from the terminal with

		rostopic pub -1 /env/situation/act justhink_msgs/Action Robot 0 "{u: 1, v: 4}"


	You can check the action types with

		rosmsg info justhink_msgs/Action


#### Published Topics

* **`/env/situation/event`** ([[justhink_msgs/StateTransition]](https://github.com/utku-norman/justhink_msgs/blob/main/msg/StateTransition.msg))

	State transitions as 3-tuples < state, action, next state >, with a header for a timestamp and an activity name, for the robot to know what is the state of the world being displayed on the application.

* **`/env/situation/onset`** ([[justhink_msgs/ActivityTransition]](https://github.com/utku-norman/justhink_msgs/blob/main/msg/ActivityTransition.msg))

	Activity changes as 2-tuples < current activity name, next activity name >, with a header that contains a timestamp and an activity name, for the robot to know that the activity being displayed on the application has changed.

* **`/env/situation/button_press`** ([[justhink_msgs/Button]](https://github.com/utku-norman/justhink_msgs/blob/main/msg/Button.msg))

	Mouse clicks on the buttons as 2-tuples < button name, button state > when its clicked (states e.g. enabled, disabled, selected), with a header that contains a timestamp and an activity name, for logging purposes.

* **`/env/situation/drawing_change`** ([[justhink_msgs/Drawing]](https://github.com/utku-norman/justhink_msgs/blob/main/msg/Drawing.msg))

	Changes in the source or destination nodes of the temporarily drawn edges, with a header that contains a timestamp and an activity name, for logging purposes. -1 indicates no source or destination node is dragged at.

* **`/env/situation/mouse_press`** ([[justhink_msgs/Mouse]](https://github.com/utku-norman/justhink_msgs/blob/main/msg/Mouse.msg))

	Mouse clicks that have position, position difference and mouse button information, with a header that contains a timestamp and an activity name, for logging purposes.

* **`/env/situation/mouse_drag`** ([[justhink_msgs/Mouse]](https://github.com/utku-norman/justhink_msgs/blob/main/msg/Mouse.msg))

	Mouse drags that have position, position difference and mouse button information, with a header that contains a timestamp and an activity name, for logging purposes.

* **`/env/situation/mouse_release`** ([[justhink_msgs/Mouse]](https://github.com/utku-norman/justhink_msgs/blob/main/msg/Mouse.msg))

	Mouse releases that have position, position difference and mouse button information, with a header that contains a timestamp and an activity name, for logging purposes.

* **`/env/situation/mouse_motion`** ([[justhink_msgs/Mouse]](https://github.com/utku-norman/justhink_msgs/blob/main/msg/Mouse.msg))

	Mouse movements that have position, position difference and mouse button information, with a header that contains a timestamp and an activity name, for logging purposes.

* **`/env/situation/key_press`** ([[justhink_msgs/Key]](https://github.com/utku-norman/justhink_msgs/blob/main/msg/Key.msg))

	Key presses on the keyboard that have the symbol and modifiers information, for logging purposes.

* **`/env/situation/key_release`** ([[justhink_msgs/Key]](https://github.com/utku-norman/justhink_msgs/blob/main/msg/Key.msg))

	Key releases on the keyboard that have the symbol and modifiers information, for logging purposes.


#### Services

* **`/env/situation/act`** ([[justhink_msgs/Act]](https://github.com/utku-norman/justhink_msgs/blob/main/srv/Act.srv))

	Executes an action in the activity for the robot to take actions like suggesting a connection etc. For example, you can execute an action from the terminal with

		rosservice call /env/situation/act "action: {agent: Human, type: 1, edge: {u: 1, v: 4}}"

	As another example, you can reset the problem state with

		rosservice call /env/situation/act "action: {agent: Human, type: 9}"

* **`/env/situation/observe_activity`** ([[justhink_msgs/ObserveActivity]](https://github.com/utku-norman/justhink_msgs/blob/main/srv/ObserveActivity.srv))

	Retrieves the name of the current activity being shown in the application. For example, you can retrieve this information from the terminal with

		rosservice call /env/situation/observe_activity

* **`/env/situation/observe_state`** ([[justhink_msgs/ObserveState]](https://github.com/utku-norman/justhink_msgs/blob/main/srv/ObserveState.srv))

	Retrieves the state of the current activity being shown in the application. For example, you can retrieve this information from the terminal with

		rosservice call /env/situation/observe_state

* **`/env/situation/pause`** ([[justhink_msgs/Pause]](https://github.com/utku-norman/justhink_msgs/blob/main/srv/Pause.srv))

	Sets the pause status of the application. If paused, no action can be taken by the human, e.g. while the robot is speaking. For example, you can pause the application from the terminal with

		rosservice call /env/situation/pause True

* **`/env/situation/set_activity`** ([[justhink_msgs/SetActivity]](https://github.com/utku-norman/justhink_msgs/blob/main/srv/SetActivity.srv))

	Sets the activity being shown in the application. For example, you can set the application to the posttest's first activity from the terminal with

		rosservice call /env/situation/set_activity 'posttest-1'

* **`/env/situation/set_robot_text`** ([[justhink_msgs/SetRobotText]](https://github.com/utku-norman/justhink_msgs/blob/main/srv/SetRobotText.srv))

	Sets the robot text being displayed in the application. For example, you can set this text from the terminal with

		rosservice call /env/situation/set_robot_text 'Hello, I am a robot!'

## Acknowledgements

This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 765955. Namely, the [ANIMATAS Project](https://www.animatas.eu/).

## Bugs & Feature Requests

Please report bugs and request features using the [Issue Tracker](https://github.com/utku-norman/screen_app/issues).


[ROS]: http://www.ros.org
[QTrobot]: https://luxai.com
[justhink_world]: https://github.com/utku-norman/justhink_world
[justhink_scenario]: https://github.com/utku-norman/justhink_scenario
[justhink_agent]: https://github.com/utku-norman/justhink_agent
[justhink_robot]: https://github.com/utku-norman/justhink_robot
[justhink_msgs]: https://github.com/utku-norman/justhink_msgs